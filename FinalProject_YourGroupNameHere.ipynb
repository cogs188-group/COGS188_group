{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 118B - Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Insert title here\n",
    "\n",
    "## Group members\n",
    "\n",
    "- Pierce Nguyen\n",
    "- Yiyi Huang\n",
    "- Xunzhi He\n",
    "- Nathalie Franklin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract \n",
    "In our project, we investigated various backtracking methods and heuristics to optimize piece placement in a simplified Tetris environment. The data for this project are dynamically generated game states that include the current board configuration along with the falling piece. We use several backtracking approaches using different heuristics such as score, score+height, full heuristics, full heuristics + alpha‐beta pruning and compare their performance to a Deep Q‐Network (DQN) model. We find that backtracking methods with domain‐specific heuristics often outperform the DQN, indicating that forward‐search and tailored board‐evaluation metrics can lead to more effective strategies. These findings are measured primarily through the Tetris score, which reflects the number of cleared lines."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "Tetris, developed by Alexey Pajitnov in 1984, has become a well-studied benchmark for artificial intelligence due to its complex decision-making process and classification as an NP-hard problem<a name=\"demaine\"></a><sup>[1]</sup>. The challenge arises from the need to efficiently place falling tetrominoes while managing an ever-changing board state.Traditional AI approaches relied on handcrafted heuristics, such as minimizing gaps and maximizing cleared lines, but these methods often struggled with long-term planning. With advancements in machine learning, reinforcement learning (RL) has emerged as a powerful technique for training AI to play Tetris. Deep Q-networks (DQN) and other deep reinforcement that utilize backtracking methods have been successfully applied to optimize decision-making in dynamic environments<a name=\"mnih\"></a><sup>[2]</sup>. Our research aims to develop a simplified Tetris AI model capable of learning optimal strategies and assisting human players. By leveraging modern AI techniques, we seek to improve decision-making efficiency and demonstrate the practical applications of reinforcement learning in structured problem domains<a name=\"browne\"></a><sup>[3]</sup>. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "\n",
    "When it comes to Tetris, the game difficulty and complexity correspond to the range of pieces, rotations and positions of placement. The larger the grid the more game variance to account for. Using traditional Tetris pieces the board must be at least 4 pieces wide and long but there is no upper limit to a Tetris grid. As we try to optimize our model, we must be able to account for how scaling up the grid affects the efficiency of our heuristic and backtracking techniques. Accounting for grid size goes hand in hand with the overall problem of creating a model that maximizes the overall score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "Since our problem is a live game, we don't have a huge dataset that we feed into the model to train it. Instead we will feed the current board state of the tetris game into our algorithm and have it pick the best moves and update the board state accordingly. Some critical variables would be the tetris board which we plan to represent as an array, the tetris pieces which we also plan to represent as an array. We also will need to account for all the possible rotations for the pieces. We will also need to make sure we update the board after each move for any cleared pieces.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proposed Solution\n",
    "\n",
    "By first minimizing the problem into 4x10 Tetris grid we aim to test how simple backtracking and heuristic can affect the overall performance of the game. Smaller games will also allow us to better visualize correlation between depth and performance. Then we can then see how scaling up the grid affects the depth required for backtracking and if the same heuristic will be sufficient. An average Tetris board is 10x20 but this allows for greater possibility of placement and our backtracking tree will scale proportionally to the width of the boards. \n",
    "\n",
    "Our proposed solution for creating an efficient backtracking algorithm to account for both variance and be runtime efficient is one with a set depth. We first load a large list of pieces that is too large to be exhausted in a reasonable game of said grid size. The branching will occur when a state of a grid (our current action space) is introduced with a new piece. Each branch on the same level accounts for a possible valid piece rotation and placement(the wider the board the more possible placements.) of the same piece. Once the set depth is reached in all branches the model will recurse backwards to the root, compare the score of all possible grid states in the leafs and make the highest scoring leaf state the current state. We will then be back to a one node tree and will restart the branching process until all branches are game over aka max height of the grid has been reached. The score and our heuristic will both be based on the rows cleared aka height. \n",
    "\n",
    "We will use this repository: https://gist.github.com/zmcedillo/012ff15aa3018cb61b45e5bafb4b15ce in order to simulate the game states and make sure that all pieces are within valid placement. Using this repository code we can also simulate boards of various sizes and can easily judge the score for scaling. Since boards of various sizes will have an expected different score, the score of each grid will be compared to an average score of the same board played 1000 times with random valid placement. We will thus compare our model's success against random models' average."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics\n",
    "\n",
    "We use the game score (rows cleared) in order to determine performance of our model. The higher the score, the more rows are cleared, the better our model has performed. As a point of comparison for our model, we will use the avegae performence  over 1000 of a random placement model. Our algorithm should be able to have a higher average score compared to this model since we are using a heuristic that focouses on maximizing score.\n",
    "\n",
    "Propose at least one evaluation metric that can be used to quantify the performance of both the benchmark model and the solution model. The evaluation metric(s) you propose should be appropriate given the context of the data, the problem statement, and the intended solution. Describe how the evaluation metric(s) are derived and provide an example of their mathematical representations (if applicable). Complex evaluation metrics should be clearly defined and quantifiable (can be expressed in mathematical or logical terms)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "You may have done tons of work on this. Not all of it belongs here. \n",
    "\n",
    "Reports should have a __narrative__. Once you've looked through all your results over the quarter, decide on one main point and 2-4 secondary points you want us to understand. Include the detailed code and analysis results of those points only; you should spend more time/code/plots on your main point than the others.\n",
    "\n",
    "If you went down any blind alleys that you later decided to not pursue, please don't abuse the TAs time by throwing in 81 lines of code and 4 plots related to something you actually abandoned.  Consider deleting things that are not important to your narrative.  If its slightly relevant to the narrative or you just want us to know you tried something, you could keep it in by summarizing the result in this report in a sentence or two, moving the actual analysis to another file in your repo, and providing us a link to that file.\n",
    "\n",
    "### Subsection 1\n",
    "\n",
    "You will likely have different subsections as you go through your report. For instance you might start with an analysis of the dataset/problem and from there you might be able to draw out the kinds of algorithms that are / aren't appropriate to tackle the solution.  Or something else completely if this isn't the way your project works.\n",
    "\n",
    "### Subsection 2\n",
    "\n",
    "Another likely section is if you are doing any feature selection through cross-validation or hand-design/validation of features/transformations of the data\n",
    "\n",
    "### Subsection 3\n",
    "\n",
    "Probably you need to describe the base model and demonstrate its performance.  Probably you should include a learning curve to demonstrate how much better the model gets as you increase the number of trials\n",
    "\n",
    "### Subsection 4\n",
    "\n",
    "Perhaps some exploration of the model selection (hyper-parameters) or algorithm selection task. Generally reinforement learning tasks may require a huge amount of training, so extensive grid search is unlikely to be possible. However expoloring a few reasonable hyper-parameters may still be possible.  Validation curves, plots showing the variability of perfromance across folds of the cross-validation, etc. If you're doing one, the outcome of the null hypothesis test or parsimony principle check to show how you are selecting the best model.\n",
    "\n",
    "### Subsection 5 \n",
    "\n",
    "Maybe you do model selection again, but using a different kind of metric than before?  Or you compare a completely different approach/alogirhtm to the problem? Whatever, this stuff is just serving suggestions.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion\n",
    "\n",
    "### Interpreting the result\n",
    "\n",
    "OK, you've given us quite a bit of tech informaiton above, now its time to tell us what to pay attention to in all that.  Think clearly about your results, decide on one main point and 2-4 secondary points you want us to understand. Highlight HOW your results support those points.  You probably want 2-5 sentences per point.\n",
    "\n",
    "\n",
    "### Limitations\n",
    "\n",
    "Are there any problems with the work?  For instance would more data change the nature of the problem? Would it be good to explore more hyperparams than you had time for?   \n",
    "\n",
    "\n",
    "### Future work\n",
    "Looking at the limitations and/or the toughest parts of the problem and/or the situations where the algorithm(s) did the worst... is there something you'd like to try to make these better.\n",
    "\n",
    "### Ethics & Privacy\n",
    "\n",
    "While a Tetris AI seems pretty simple and narrow in scope it can have some ethical implications. Tetris is mostly a singleplayer game but tournaments and leaderboards do exist so its possible for someone to cheat using this AI to gain an unfair advantage. Looking through Deon's ethical checklist, since we aren't utilizing any dataset we don't need to worry about any biases or gaps in our data. Our code is unlikely to be used for a different purpose than playing Tetris but even if it is it will only be able to be used to predict where the best place to place a block on a gameboard is so there is a low potential for misuse/harm.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Reiterate your main point and in just a few sentences tell us how your results support it. Mention how this work would fit in the background/context of other work in this field if you can. Suggest directions for future work if you want to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Footnotes\n",
    "<a name=\"demainenote\">[1]</a> Demaine, E. D., Hohenberger, S., & Liben-Nowell, D. (2003). Tetris is hard, even to approximate. Computational Complexity, 13(4), 426-453.<a name=\"mnihnote\">[2]</a> Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., et al. (2015). Human-level control through deep reinforcement learning. Nature, 518(7540), 529-533.<a name=\"brownenote\">[3]</a> Browne, C. B., Powley, E., Whitehouse, D., Lucas, S. M., et al. (2012). A survey of Monte Carlo Tree Search methods. IEEE Transactions on Computational Intelligence and AI in Games, 4(1), 1-43."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11 (default, Jul 27 2021, 07:03:16) \n[Clang 10.0.0 ]"
  },
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 118B - Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tetris AI Backtracking Comparison\n",
    "\n",
    "## Group members\n",
    "\n",
    "- Pierce Nguyen\n",
    "- Yiyi Huang\n",
    "- Xunzhi He\n",
    "- Nathalie Franklin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract \n",
    "This section should be short and clearly stated. It should be a single paragraph <200 words.  It should summarize: \n",
    "- what your goal/problem is\n",
    "- what the data used represents \n",
    "- the solution/what you did\n",
    "- major results you came up with (mention how results are measured) \n",
    "\n",
    "__NB:__ this final project form is much more report-like than the proposal and the checkpoint. Think in terms of writing a paper with bits of code in the middle to make the plots/tables\n",
    "\n",
    "-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "In our project, we investigated various backtracking methods and heuristics to optimize piece placement in a simplified Tetris environment. The data for this project are dynamically generated game states that include the current board configuration along with the falling piece. We use several backtracking approaches using different heuristics such as score, score+height, full heuristics, full heuristics + alpha‐beta pruning and compare their performance to a Deep Q‐Network (DQN) model. We find that backtracking methods with domain‐specific heuristics often outperform the DQN, indicating that forward‐search and tailored board‐evaluation metrics can lead to more effective strategies. These findings are measured primarily through the Tetris score, which reflects the number of cleared lines."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "Tetris has long been seen as an NP‐hard problem<a name=\"demaine\"></a><sup>[1]</sup>, making it a strong benchmark for AI research. Early strategies for Tetris often relied on handcrafted heuristics such as penalizing holes or high stacks to place pieces effectively. However, these heuristic approaches can struggle with looking further ahead, prompting the use of search methods such as backtracking and alpha‐beta pruning. Meanwhile, reinforcement learning, especially Deep Q‐Networks (DQN), has been applied to a range of complex control tasks, including some variants of Tetris<a name=\"mnih\"></a><sup>[23]</sup>. Despite successes in other domains, DQN often faces challenges with Tetris’s sparse rewards and large state space, highlighting the continued importance of search‐based techniques. By comparing backtracking algorithms with and without alpha‐beta pruning to a DQN approach, we aim to clarify how heuristic search can exploit domain knowledge more efficiently than purely data‐driven methods.\n",
    "\n",
    "<br> <a name=\"demainenote\">[1]</a> Demaine, E. D., Hohenberger, S., & Liben‐Nowell, D. (2003). *Tetris is hard, even to approximate.* Computational Complexity, 13(4), 426–453. <a name=\"mnihnote\">[2]</a> Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., et al. (2015). *Human‐level control through deep reinforcement learning.* Nature, 518(7540), 529–533."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "\n",
    "Clearly describe the problem that you are solving. Avoid ambiguous words. The problem described should be well defined and should have at least one ML-relevant potential solution. Additionally, describe the problem thoroughly such that it is clear that the problem is quantifiable (the problem can be expressed in mathematical or logical terms), measurable (the problem can be measured by some metric and clearly observed), and replicable (the problem can be reproduced and occurs more than once).\n",
    "\n",
    "-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "The goal of this project is to develop an AI that optimally places pieces in a Tetris game to maximize the game score, defined as the total number of cleared lines.\n",
    "\n",
    "This problem is quantifiable because the performance metric can be expressed based on the number of cleared rows and penalized by board features such as aggregate height, holes, and bumpiness. It is measurable since each game run produces a score and a number of moves, which serve as clear indicators of performance. Moreover, the problem is replicable because the game environment can be reset to a predefined state, and the same sequence of tetrominoes can be used across different trials.\n",
    "\n",
    "Our approach compares machine learning methods—specifically Deep Q-Networks (DQN)—with search-based methods (backtracking, with and without alpha-beta pruning) that use domain-specific heuristics. This comparison addresses the challenges of sparse rewards in DQN and evaluates whether heuristic search can more consistently yield higher scores in this NP-hard problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "Since our problem is a live game, we don't have a huge dataset that we feed into the model to train it. Instead we will feed the current board state of the tetris game into our algorithm and have it pick the best moves and update the board state accordingly. Some critical variables would be the tetris board which we plan to represent as an array, the tetris pieces which we also plan to represent as an array. We also will need to account for all the possible rotations for the pieces. We will also need to make sure we update the board after each move for any cleared pieces.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proposed Solution\n",
    "\n",
    "In this section, clearly describe a solution to the problem. The solution should be applicable to the project domain and appropriate for the dataset(s) or input(s) given. Provide enough detail (e.g., algorithmic description and/or theoretical properties) to convince us that your solution is applicable. Make sure to describe how the solution will be tested.  \n",
    "\n",
    "If you know details already, describe how (e.g., library used, function calls) you plan to implement the solution in a way that is reproducible.\n",
    "\n",
    "If it is appropriate to the problem statement, describe a benchmark model<a name=\"sota\"></a>[<sup>[3]</sup>](#sotanote) against which your solution will be compared. \n",
    "\n",
    "-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "Our solution employs a dual approach to address optimal piece placement in Tetris. First, we implement a search-based method using a backtracking algorithm with fixed depth. In this method, a list of upcoming pieces is preloaded and, for each piece, all legal moves are generated using functions like `get_legal_placements` and `simulate_placement`. The algorithm recursively explores these moves up to a set depth, evaluating the resulting board states with a heuristic that combines the game score, aggregate height, number of holes, and bumpiness. Two variants of the backtracking approach are implemented: one using full heuristics without alpha-beta pruning and another incorporating alpha-beta pruning to efficiently cut off suboptimal branches. We also tested out much more basic heuristics such as just score and score + height to see how the heuristics impact the average score. The output for each variant is recorded in CSV files which log the game number, number of moves, and final score.\n",
    "\n",
    "TODO: talk about DQN and comparison\n",
    "\n",
    "TODO: potentially add equations/snippets of code (mainly heuristics such as `evaluate_state`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics\n",
    "\n",
    "We use the average final score for each  as the primary evaluation metric. For each algorithm, we run 100 games—each terminated after 150 moves. We did this because our full heuristics model did not lose even after running for an hour which would make running 100 games impossible. The average final score is calculated by summing the final scores from all games and then dividing by the total number of games. This metric directly quantifies how effectively an algorithm can clear lines and achieve high scores. The score is calculated according to Tetris' rules with lines cleared as the only contributing factor to the overall score.\n",
    "\n",
    "Note: eval metrics are strictly in relation to how we compare our different algos not math behind backtracking/heuristics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "`TODO`\n",
    "\n",
    "You may have done tons of work on this. Not all of it belongs here. \n",
    "\n",
    "Reports should have a __narrative__. Once you've looked through all your results over the quarter, decide on one main point and 2-4 secondary points you want us to understand. Include the detailed code and analysis results of those points only; you should spend more time/code/plots on your main point than the others.\n",
    "\n",
    "If you went down any blind alleys that you later decided to not pursue, please don't abuse the TAs time by throwing in 81 lines of code and 4 plots related to something you actually abandoned.  Consider deleting things that are not important to your narrative.  If its slightly relevant to the narrative or you just want us to know you tried something, you could keep it in by summarizing the result in this report in a sentence or two, moving the actual analysis to another file in your repo, and providing us a link to that file.\n",
    "\n",
    "### Comparing Various Backtracking Algorithms and Heuristics\n",
    "`TODO:` Fill this out\n",
    "\n",
    "You will likely have different subsections as you go through your report. For instance you might start with an analysis of the dataset/problem and from there you might be able to draw out the kinds of algorithms that are / aren't appropriate to tackle the solution.  Or something else completely if this isn't the way your project works.\n",
    "\n",
    "### Comparing Backtracking vs DQN\n",
    "`TODO:` Fill this out\n",
    "Note: only compare using the best backtracking algo not the worse 3\n",
    "\n",
    "Another likely section is if you are doing any feature selection through cross-validation or hand-design/validation of features/transformations of the data\n",
    "\n",
    "### Comparing Backtracking vs Random\n",
    "Note: idk if we need this probably not\n",
    "\n",
    "Probably you need to describe the base model and demonstrate its performance.  Probably you should include a learning curve to demonstrate how much better the model gets as you increase the number of trials\n",
    "\n",
    "-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "Note: add more subsections if you think we need them but idk what other results we have\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion\n",
    "\n",
    "### Interpreting the result\n",
    "`TODO`\n",
    "\n",
    "OK, you've given us quite a bit of tech informaiton above, now its time to tell us what to pay attention to in all that.  Think clearly about your results, decide on one main point and 2-4 secondary points you want us to understand. Highlight HOW your results support those points.  You probably want 2-5 sentences per point.\n",
    "\n",
    "\n",
    "### Limitations\n",
    "\n",
    "Are there any problems with the work?  For instance would more data change the nature of the problem? Would it be good to explore more hyperparams than you had time for?   \n",
    "\n",
    "One of our major limitations was computing power. Due to limited resources we could not explore a depth lower than 3 since it would take forever so we couldn't use alpha beta pruning to its full extent. Also obviously with an increased depth we likely would have had a higher average score for our backtracking section. Another major limitation would be that we used a simplified version of tetris that doesn't have time scaling, the full block set, or as advanced piece movement (sliding pieces under each other etc). We used this simplified version to make it easier to integrate our algorithms into the tetris game.\n",
    "\n",
    "### Future work\n",
    "`TODO`\n",
    "\n",
    "Looking at the limitations and/or the toughest parts of the problem and/or the situations where the algorithm(s) did the worst... is there something you'd like to try to make these better.\n",
    "\n",
    "### Ethics & Privacy\n",
    "`TODO` prob can copy paste from proposal\n",
    "\n",
    "If your project has obvious potential concerns with ethics or data privacy discuss that here.  Almost every ML project put into production can have ethical implications if you use your imagination. Use your imagination.\n",
    "\n",
    "Even if you can't come up with an obvious ethical concern that should be addressed, you should know that a large number of ML projects that go into producation have unintended consequences and ethical problems once in production. How will your team address these issues?\n",
    "\n",
    "Consider a tool to help you address the potential issues such as https://deon.drivendata.org\n",
    "\n",
    "### Conclusion\n",
    "`TODO`\n",
    "\n",
    "Reiterate your main point and in just a few sentences tell us how your results support it. Mention how this work would fit in the background/context of other work in this field if you can. Suggest directions for future work if you want to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br> <a name=\"demainenote\">[1]</a> Demaine, E. D., Hohenberger, S., & Liben‐Nowell, D. (2003). *Tetris is hard, even to approximate.* Computational Complexity, 13(4), 426–453. <a name=\"mnihnote\">[2]</a> Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., et al. (2015). *Human‐level control through deep reinforcement learning.* Nature, 518(7540), 529–533."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "A4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
